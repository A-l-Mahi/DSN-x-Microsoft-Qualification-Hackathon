{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Zc9BNxQFSPMw",
        "T3a3mY-AaFQF",
        "nfftIs4azETp",
        "feZfj4iugOj6",
        "quF1M8mRidx2",
        "HEdCJOgYYJlY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset split dataset into train and CV"
      ],
      "metadata": {
        "id": "Zc9BNxQFSPMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q catboost\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import  accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qIbLmgUDSy7k",
        "collapsed": true
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Yd5G5FDAN-7n"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/DSN Quali\"\n",
        "\n",
        "train_set = pd.read_csv(os.path.join( BASE_PATH, \"train-03-06.csv\"))\n",
        "test_set = pd.read_csv(os.path.join( BASE_PATH, \"test-03-06.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ],
      "metadata": {
        "id": "T3a3mY-AaFQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set['Age_MaxHR'] = train_set['Age'] * train_set['thalach']\n",
        "train_set['Cholesterol_RestingBP'] = train_set['chol'] * train_set['trestbps']\n",
        "train_set['Age_RestingBP'] = train_set['Age'] * train_set['trestbps']\n",
        "\n",
        "train_set['BP_Cholesterol_Ratio'] = train_set['trestbps'] / train_set['chol']\n",
        "train_set['HR_Age_Ratio'] = train_set['thalach'] / train_set['Age']\n",
        "\n",
        "train_set['Cardio_Load'] = train_set['trestbps'] + train_set['thalach']\n",
        "\n",
        "train_set['Log_Cholesterol'] = np.log1p(train_set['chol'])\n",
        "train_set['Log_Oldpeak'] = np.log1p(train_set['oldpeak'])\n",
        "\n",
        "train_set['sqrt_MaxHR'] = np.sqrt(train_set['thalach'])\n",
        "\n",
        "train_set['HR_Reserve'] = 220 - train_set['Age'] - train_set['thalach']\n"
      ],
      "metadata": {
        "id": "q08ql0x2aMYO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering on  test set\n",
        "test_set['Age_MaxHR'] = test_set['Age'] * test_set['thalach']\n",
        "test_set['Cholesterol_RestingBP'] = test_set['chol'] * test_set['trestbps']\n",
        "test_set['Age_RestingBP'] = test_set['Age'] * test_set['trestbps']\n",
        "\n",
        "test_set['BP_Cholesterol_Ratio'] = test_set['trestbps'] / test_set['chol']\n",
        "test_set['HR_Age_Ratio'] = test_set['thalach'] / test_set['Age']\n",
        "\n",
        "test_set['Cardio_Load'] = test_set['trestbps'] + test_set['thalach']\n",
        "\n",
        "test_set['Log_Cholesterol'] = np.log1p(test_set['chol'])\n",
        "test_set['Log_Oldpeak'] = np.log1p(test_set['oldpeak'])\n",
        "\n",
        "test_set['sqrt_MaxHR'] = np.sqrt(test_set['thalach'])\n",
        "\n",
        "test_set['HR_Reserve'] = 220 - test_set['Age'] - test_set['thalach']"
      ],
      "metadata": {
        "id": "zmd6EVsgvNWf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_set.columns = train_set.columns[:-1]\n",
        "cat_variables = [\"age_clusters\",\n",
        "                 \"trestbps_clusters\",\n",
        "                 \"chol_clusters\",\n",
        "                 \"thalach_clusters\",\n",
        "                 \"restecg\",\n",
        "                 \"cp\",\n",
        "                 \"fbs\",\n",
        "                 \"exang\",\n",
        "                 \"slope\",\n",
        "                 \"ca\",\n",
        "                 \"thal\"\n",
        "                 ]\n",
        "train_set = pd.get_dummies(data = train_set,\n",
        "                         prefix = cat_variables,\n",
        "                         columns = cat_variables)\n",
        "test_set = pd.get_dummies(data = test_set,\n",
        "                         prefix = cat_variables,\n",
        "                         columns = cat_variables)"
      ],
      "metadata": {
        "id": "EJkCWrhbw7eC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get important features"
      ],
      "metadata": {
        "id": "nfftIs4azETp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_positives = ['exang_1', 'cp_0', 'HR_Reserve', 'thalach', 'Sex', 'oldpeak', 'thal_1', 'ca_0', 'Age', 'Log_Oldpeak', 'Id', 'target']\n",
        "\n",
        "train_set  =   train_set[top_10_positives]\n",
        "test_set = test_set[top_10_positives[:-1]]"
      ],
      "metadata": {
        "id": "Cjakkpr50ddP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(train_set.drop(['Id','target'], axis=1), train_set['target'], train_size = .8, random_state=42)"
      ],
      "metadata": {
        "id": "zOlhrPF7PqhA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithms"
      ],
      "metadata": {
        "id": "feZfj4iugOj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logit_reg(X_train, y_train):\n",
        "    log_reg = LogisticRegression(\n",
        "        max_iter=100,          # Increase max_iter if you encounter convergence issues\n",
        "        solver='lbfgs',       # Optimization algorithm\n",
        "        C=206.913808111479,                # Inverse regularization strength\n",
        "        penalty='l2',         # Regularization type\n",
        "        random_state=42       # For reproducibility\n",
        "    )\n",
        "\n",
        "    log_reg.fit(X_train, y_train)\n",
        "    return log_reg\n",
        "\n",
        "def random_forest(X_train, y_train):\n",
        "    random_forest = RandomForestClassifier(\n",
        "        n_estimators= 50,       # Number of trees in the forest\n",
        "        criterion= 'entropy',     # or 'entropy'\n",
        "        max_depth= 10,          # Limit the depth of the trees\n",
        "        min_samples_split=2,     # Minimum number of samples required to split an internal node\n",
        "        min_samples_leaf=1,      # Minimum number of samples required to be at a leaf node\n",
        "        random_state=42          # For reproducibility\n",
        "    )\n",
        "    random_forest.fit(X_train, y_train)\n",
        "    return random_forest\n",
        "\n"
      ],
      "metadata": {
        "id": "htZTXLkBgSFC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call all algorithms"
      ],
      "metadata": {
        "id": "quF1M8mRidx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call all defined algorithms into a dictionary1.0\n",
        "algos = {\"logistic\": logit_reg(X_train, y_train), \"random_rf\": random_forest(X_train, y_train)}"
      ],
      "metadata": {
        "id": "irMKuFlWinud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8febd8-1357-4227-c760-2b3d7b932979"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in algos:\n",
        "#    print(algos[i])\n",
        "    y_pred = algos[i].predict(X_train)\n",
        "    y_pred_val = algos[i].predict(X_val)\n",
        "\n",
        "    print(f\"{i} Train: {accuracy_score(y_train, y_pred)}  Val: {accuracy_score(y_val, y_pred_val)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVvhkil7jeUH",
        "outputId": "c013e5ee-0408-411a-f09a-16d465a7454b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logistic Train: 0.8195823348168435  Val: 0.8254620123203286\n",
            "random_rf Train: 0.921773365285861  Val: 0.8234086242299795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run prediction"
      ],
      "metadata": {
        "id": "HEdCJOgYYJlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test_set.columns = train_set.columns[:-1]\n",
        "#test_set = test_set[top_10_positives[:-1]]\n",
        "for i in algos:\n",
        "    y_pred = algos[i].predict(test_set.drop(['Id'], axis=1))\n",
        "\n",
        "    submission = pd.DataFrame({'Id': test_set['Id'], 'target': y_pred})\n",
        "    submission.to_csv(f\"submission_{i}.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "FFvG-N0Xjlwj"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}